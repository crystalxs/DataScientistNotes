# 1.4. Random Forest

1. Bootstrap sample: the default is to have the randomly selected data be the same size as the initial dataset.
2. Build a Decision Tree with each new resample, and at each split bootstrap variables.
3. Vote: probably the most common ensemble method.

## Adavantage

- Same advantages as Decision Trees
- Robust to overfitting
- Robust to missing variables
- Easy to interpret
- Often perform very well with almost no parameter tuning
- Performance is among the best for traditional ML algorithms
- Can be parallelized for distributed computing

## When should we not use RF

Small data, especially when the total n is small.

Also if we need every feature to learn (i.e., extremely heterogeneous data).

## Training

Limit each node of the Decision Tree to only consider splitting on a random subset of the features.

Since trees are greedy, each split would always choose the best split. By randomly sampling downsampling from features, we limit how good each individual split is.

Slide Type-SlideSub-SlideFragmentSkipNotes

###Reasonable defaults for feature limits (m)

Classification: ùëö=ùëù‚éØ‚éØ‚àöm=p

Regression: ùëö=ùëù3m=p3

where p is the total number features

m is a hyperparameter that can be learned

## Prediction

To classify a new datapoint, use each tree in the forest to get a prediction.

Choose the category that gets the most votes.

## Sklearn

https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html

```python
from sklearn.ensemble import RandomForestClassifier

features_train, labels_train, features_test, labels_test
rf = RandomForestClassifier()
rf.fit(features_train, labels_train)
rf.score(features_test, labels_test)
```

### Feature importance

If a features is higher in a tree, the more important it is in classifying the data.

The expected fraction of data points that reach a node can be used as an estimate of that feature's importance for that tree.

Those values can be averaged across all trees to get the feature's importance.

Given that each feature is measured on a ratio scale, we are able to the relative percentages or one feature 'x' times better than another feature.

```python
feature_importance = dict(zip(iris.feature_names, rf.feature_importances_))
for name, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):
    print(f"{name:<17}: {importance:>6.2%}")
```

##Out-of-Bag Error

`rf.oob_score_ `

The mean prediction error on each training sample, using only the trees that did **not** have the data in their bootstrap sample.

Can be used to measure the prediction error for any bagging method.

### Why useful

Typically we do a three-way split (train, validation, and test).

OOB gives use another method to assess our generalization error.

### When should we not use OOB

Comparing across algorithms that are not bagged with the same procedure.

For example, use CV to compare Random Forest‚Ñ¢ to k-NN.

## Random Forest Regression

Random forest can learn arbitrary relationships between the features and the outcome, even non-monotonic relationships.

Simple Linear Regression models only monotonic relationships.

###When is a Linear Regression (LR) better than a Random Forest‚Ñ¢?

- With very small data (5-100 examples), RF is more likely to over-fit than LR.
- When the underlying relationship is truly linear, LR will be a more parsimoniousness model.
- LR might train faster than RF:
  - OLS is much faster than recursive partitioning.
  - SGD is often faster than recursive partitioning.