# 1.3. Decision Tree

supervised, non-parametric

A series of binary splits that define a path which leads to an outcome.

## terminology

Root: located on the top, the first split.

Node: do a binary split on a feature based a specific value of a specific attribute to minimize entropy.

Leaf (perminal node): classifies **all** members in the defined feature space as belonging to a **single** class (final prediction) (decision boundary).

## Advantages

- Simple to use (explain and visualize)
- DT requires little data preparation
- Handle numerical and categorical data
- Handle many types of outputs (regression and multi-class classification)

## Disadvantage

* **Easily overfit**

* Cannot deal with complex data

* Cannot deal with unbalanced data
  * DT is low bias, thus might overfit the most frequent class.

### Avoiding overfit

1. Acquire more training data

2. Pruning

   * Pruning of the decision tree is done by replacing a whole subtree by a leaf node.

   * The replacement takes place if a decision rule establishes that the expected error rate in the subtree is greater than in the single leaf.
   * In the applied setting, it is better to prefer pre-prune. It is faster because it requires fewer passes over the data.

3. Setting the maximum depth of the tree

4. Setting the minimum number of samples required at each leaf node

5. Setting how much IG required to make another split

# Handle build a Decision Tree

Decision Trees learn **top-down, greedily, recursively**.

### Divide-and-conquer

Split the data into subsets, which are then split repeatedly into even smaller subsets, …

The process stops when the algorithm determines the data within the subsets are sufficiently homogeneous (or another stopping criterion has been met).

### Top-down

From a single split.

### Greedily

Choose the split that has results in the most pure feature space (current best value).

Greedy algorithms can have local minimums. There could be complex / future choice dependent rules that could perform better.

###Stop

The goal of DT is divide the dataset into pure regions.

Slide Type-SlideSub-SlideFragmentSkipNotes

1. All instances are in same class (simplest and most often used)
2. The number of instances is less than a specified minimum
3. The tree is deeper than some minimum
4. The improvement of class impurity is less than a specified minimum

### Step

1. Start with a single leaf node containing all data
2. Search for a decision rule for a single value on a single dimensions that maximally reduces the entropy
   1. Calculate the entropy of the dataset
   2. For every feature, calculate the entropy for all values
   3. Pick feature and value that has the highest Information Gain
3. Stop based on criteria

## Data Preparation

- data normalization
- dummy variables need to be created
- blank values to be removed

## Feature Selection

### Multi-nominal feature

1. Consider all possible pairs of splits. For k classes there are 2k-1-1 splits, which is computationally prohibitive if k is a large number.
2. If there are ordered classes, then we do version of binary search. This reduces the search to k-1 possible splits for k classes.
3. If there are have many ks, a One-Versus-All-The-Rest reduces the search to linear for number of k.

### Continuous feature

We need a threshold split (i.e., x ≤ 42)

One option is too sort the data and perform binary search for optional split value.

This could be computational intractable because it requires sorting O(nlogn) then binary search O(logn).

###Methods

Aka: measure purity of a feature space

- Classification Error
- Information Gain
- Gini Index

- Gain ratio
- Twoing criteria
- Distance Measure
- Likelihood-Ratio Chi–Squared Statistics
- DKM Criterion
- Normalized Impurity Based Criteria
- Binary Criteria
- Orthogonal (ORT) Criterion
- Kolmogorov–Smirnov Criterion
- AUC–Splitting Criteria

####Classification Error 

$\text{𝐶𝑙𝑎𝑠𝑠𝑖𝑓𝑖𝑐𝑎𝑡𝑖𝑜𝑛 𝐸𝑟𝑟𝑜𝑟}=1−𝑚𝑎𝑥𝑝_𝑘$

where $𝑝_𝑘$ denotes the proportion of instances belonging to class $𝑘$.

####Information Gain

aka, KL Divergence or Relative Gain or Relative Entropy.

**Choose one split over another**: find the attribute that returns the highest information gain (i.e., the most homogeneous branches).

$\text{Information Gain} = \text{entropy}(parent) - \text{weighted average of entropy}(children)$

Always choose the largest Information Gain.

####Gini Index

A measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.

Computed by summing the probability $𝑝_𝑖$ of an item with label $𝑖$ being chosen times the probability $\sum_{𝑖 \neq 𝑖}𝑝_𝑘=1−𝑝_𝑖$ of a mistake in categorizing that item.

$Gini = 1 - \sum_{i=1}^K p_k^2$

Properties:

- Maximized when elements are heterogeneous (impure).
- Minimized when elements are homogenous (pure).

#### Limitations

- Classification Error
  - Bad idea (remember the problems with accuracy)

- Information Gain
  - Favors many valued features (e.g., multinomial or continuous). Since All things being equal, entropy is higher in those cases, Information Gain is Bias towards many valued features (e.g., multinomial or continuous). 

- Gini Index
  - Favors equal-sized partitions with purity.
  - Has difficulties when the number of classes is large.

## How would we split for multi-nominal features?

1. Consider all possible pairs of splits. For k classes there are 2k-1-1 splits, which is computationally prohibitive if k is a large number.
2. If there are ordered classes, then we do version of binary search. This reduces the search to k-1 possible splits for k classes.
3. If there are have many ks, a One-Versus-All-The-Rest reduces the search to linear for number of k.

## How would we split for continuous features?

We need a threshold split (i.e., x ≤ 42)

One option is too sort the data and perform binary search for optional split value.

This could be computational intractable because it requires sorting O(nlogn) then binary search O(logn).

## Sklearn

https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree

```python
from sklearn import tree
# from sklearn.tree import DecisionTreeClassifier

features_train, labels_train, features_test, labels_test
clf = tree.DecisionTreeClasifier()
clf = clf.fit(features_train, labels_train)
clf.predict(features_test)
```

### Tunning the pararmeters

https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html

```python
class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)
```

`min_samples_split`: minimum number of elements in each leaf.

`min_samples_split` vs overfitting:

### Plot Decision Tree

```python
from IPython.display import Image  
from sklearn.externals.six import StringIO  
from sklearn.tree import export_graphviz
import pydotplus

dot_data = StringIO()
export_graphviz(dt, out_file=dot_data,  
                filled=True, rounded=True, special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
Image(graph.create_png())
```

### `tree_` Attribute

The decision estimator has an attribute called tree_ which stores the entire tree structure and allows access to low level attributes.

```python
# number of node in the dt
n_nodes = dt.tree_.node_count
# id of the left child of the node
children_left = dt.tree_.children_left
# id of the right child of the node
children_right = dt.tree_.children_right
# feature used for splitting the node
feature = dt.tree_.feature
# threshold value at the node
threshold = dt.tree_.threshold
```

## Type

### Classification Trees

- Predict: class label
- Training: minimize impurity
- Inference: assign majority class label of leaf node

### Regression Trees

- Predict: value
- Training: minimize error (typically mean squared)
- Inference: mean of the leaf node



