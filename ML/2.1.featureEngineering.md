# 2.1. Feature Engineering

Representing data is the best way possible for a ML algorithm.

The process of formulating the most appropriate features given the goal, the algorithm, and the raw data.

## Approach

1. Hand crafted rules
2. Learned models
3. Stacking

### Hand Crafted rules

A good place to start

Very common

Requires domain expertise

### Learned

Apply ML to Feature Engineering, typically unsupervised (e.g., dimension reduction or clustering)

### Stacking (ensembling)

The outputs of one model become the inputs of another model

```python
Pipeline = [Transformer, Transformer, Transformer]
```

## Techniques

- Handling Missing Values
- Vectorizing
- Filtering / Thresholding
- Binning
- Transforming
- Feature selection

### Missing Values

Be very careful that the missing data is not systematic of the effect you are studying.

Maybe missing data could be a feature (not a bug).

Slide Type-SlideSub-SlideFragmentSkipNotes

####(An incomplete) list of techniques:

- Drop rows (instances)

- Drop columns (features)

- Impute values

  * Go get the missing data

  * Sample from existing values

  * Calculated the central tendency of existing values (mean, median, mode)

  * Fit a model on other features to estimate missing value
    * [Regression](https://en.wikipedia.org/wiki/Imputation_(statistics)#Regression) is often used because it is multivariate mean estimation.
    * [k-NN works well](http://conteudo.icmc.usp.br/pessoas/gbatista/files/his2002.pdf)

  * Deep Learning is showing promise as a Pattern Matching tool.

### Vectorizing

Categorical (nominal or ordinal) needs to be transformed to a reasonable numbers for ML algorithms to be applied.

#### Methods

- One-Hot Encoding

- Dummy Coding

- Feature Hashing:

  A hash function is a deterministic function that maps potentially unbounded data to a finite integer range [1, m]. Fit a model on the hash indexes as features. The raw data is vectorized and compressed but loses all interpretability.

- Bin Counting:

  Rather than using the value of the categorical variable as the feature, instead use the **conditional probability of the target under that value**. Turns a large, sparse, binary representation of the categorical variable (e.g., one-hot encoding) into a very small, dense, real-valued numeric representation.

- Embeddings:

  If you have any sequential discrete data, embedded it. Examples: Words, emojis, website browsing, images, videos, product purchasing â€¦

| Methods          | Size requirement                        | Growth                   |
| ---------------- | --------------------------------------- | ------------------------ |
| One-Hot Encoding | k # of categories                       | grow with new categories |
| Dummy Coding     | k-1 # of categories                     | grow with new categories |
| Feature Hashing  | m hash table size                       | fixed                    |
| Bin Counting     | m bin table size                        | fixed                    |
| Embedding        | d number of dimensions of feature space | fixed                    |

### Filtering / Thresholding

Always perform EDA (especially univariate).

Remove out-of-bound values

* Prevent out-of-bound values
* Defining outliers / anomaly
  * Outliers generated by the same statistical process as your data (just unusual spread)
  * Outliers generated by a **different** statistical process as your data

### Binning, aka Quantization

Discretize continuous values into a smaller number of "bins".

Fitting a model to bins reduces the impact that small fluctuates in the data has on the model, often small fluctuates are just noise. Each bin "smooths" out the fluctuates/noises in sections of the data.

#### Why

1. It makes sense for your goal (e.g., categorize people's age by decade for marketing)
2. Improve signal-to-noise ratio (e.g., aggregate GPS data)

###Transforming

####Linear

- Most commonly each data value is added or multiplied by the same constant
- Example: Normalization & Standardization
- Generally fine
- Linear preserves the operations of addition and scalar multiplication

####Nonlinear

- Mostly commonly each data value is added or multiplied by a different value
- Examples: Squaring each value
- "With great power, comes great responsibility"
- Nonlinear does **NOT** preserves the operations of addition and scalar multiplication

####Rescaling

Often features are orders of magnitude different from each other.

Several ML algorithms are sensitive to feature scaling:

- K-NN
- SVM
- neural network

#####Normalization vs standardization

| Normalization (min-max scaling)                              | Standardization                      |
| ------------------------------------------------------------ | ------------------------------------ |
| $x = \frac{x-min(x)}{max(x)-min(x)}$                         | $x_{changed} = \frac{x-\mu}{\sigma}$ |
| Rescales the values into a range of [0,1]                    |                                      |
| outliers from the data set are compressed                    | Retains outlier values               |
| Useful where all values need to be on the same positive scale |                                      |

###Feature selection

#### Methods

1. Filtering
2. Wrapper methods
3. Embedded methods