# Information Theory



### Information Theory \(IT\) <a id="Information-Theory-(IT)"></a>

![](https://www.researchgate.net/profile/Fareed_Al-Hindawi/publication/313928362/figure/fig2/AS:493421879140353@1494652351277/Figure-no-2-Shannon-Weaver-Information-Theory-1949-This-diagram-refers-to-the.png)

The goal of IT is to define the fundamental limits on signal processing and communication, such as data compression.

### Information Theory Greatest Hits <a id="Information-Theory-Greatest-Hits"></a>

* Modern computers
* Internet
* Telecommunications systems, including mobile phones
* Computational linguists modeling
* Understanding of black holes
* Voyager missions to deep space

### **Why do we care about IT?**[**¶**](http://localhost:8888/notebooks/06_information_theory__decision_trees_I/1_it_intro_%26_entropy.ipynb#Why-do-we-care-about-IT?)\*\*\*\*

![](https://bricaud.github.io/personal-blog/images/entropy/splitdiagram.png)

Information Theory is how decision tree actually decide.

### Entropy in Information Theory

Claude Shannon defined the fundamental units of **information**, the smallest possible chunk that cannot be divided any further.

He coined the "bits". Bit is short for binary digit: 0 or 1.

Groups of bits which can be used to encode **any** message.

All messages can be digitized, hence the digital revolution.

### Shannon entropy is the quantity 𝐻H <a id="Shannon-entropy-is-the-quantity-$H$"></a>

𝐻=−∑𝑝\(𝑥\)𝑙𝑜𝑔𝑝\(𝑥\)H=−∑p\(x\)logp\(x\)

Sum up the probabilities of the all possible symbols \(x\) that might turn up in a message, weighted by the number of bits needed to represent the value of that symbol \(x\).

