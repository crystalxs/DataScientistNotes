# 3.0. Spark SQL

## Initiate SparkSession

The entry point to programming Spark with the DataFrame API.

```python
from pyspark.sql import SparkSession
ss = SparkSession.builder.getOrCreate()
```

## Creating DataFrame

* Convert existing RDDs.

  * `.toDF()`: using RDDs containing row data as tuples.

    * `.toDF()`.
    * `.toDF(columnnames)`, where `columnnames = ['column1', 'column2', ...]`.
    * All the columns are string type.
    * All the columns are nullable.

  * `.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)`: specifying a schema.

    * `data`: an RDD of row/tuple/list/dict, list or pandas.DataFrame.

    * `schema`: a StructType or list of column names.

      * `schema = StructType([StructField("colomn1", DataType, True/False), ...])`
      * `StructType`: Representing rows. Consisting of a list of StructField.
      * `StructField`: Representing columns. Including column name(string), data type, nullable(default: True), metadata(default: None). 
        * Need to add `from pyspark.sql.types import *`
        * DataType: `NullType()`, `StringType()`, `BinaryType()`, `BooleanType()`, `DateType()`, `TimstampType()`,
          `DoubleType()`, `FloatType()`, `IntegerType()`, `ArrayType()` etc.

    * `samplingRatio`: the sample ratio of rows used for inferring the schema.

    * `verifySchema`: verify data types of every row against schema. 

    * Fix error: in case there are non-integer type to be converted.

      ```python
      def IntegerSafe(value):
          try:
              return int(value)
          except ValueError:
              return None
      ```

* By using SparkSQL

  * JSON: `ss.read.json(<file_name>, schema, etc)`.

  * CSV: `ss.read.csv(<file_name>, schema, etc)`.

  * Relational database and other DB with JDBC:

    * Provide the `mongo-spark-connector` package as a packages parameter value:

    * ```python
      import os
      pyspark_submit_args = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0 pyspark-shell'
      os.environ["PYSPARK_SUBMIT_ARGS"] = pyspark_submit_args
      ```

    * Provide `spark.mongodb.input.uri` and its URI, DB and collection:

    * ```python
      spark = SparkSession \
          .builder \
          .appName("ex14") \
          .config("spark.mongodb.input.uri", "mongodb://127.0.0.1/msds694.business")\
          .getOrCreate()
      ```

    * `ss.read.format("com.mongodb.spark.sql.DefaultSource").load()`

**Checking the schema using `.printSchema()`.**

**Checking the data frame using `.show(num_rows)`.**

## Registering DataFrame in the Table Catalog

* You can reference a DataFrame by its name by registering the DataFrame as a table.
* Spark stores the table definition in the table catalog.
* Save as table: `.write.saveAsTable(<table_name>)`
  * Register a table permanently.
  * The table definitions will survive your applicationâ€™s restarts and are persistent 
* Once DataFrame is registered as a table, you can query its data using SQL expressions.
  * **`ss.sql(sql_query)`**

## DateFrame API

* `select(<column names>/list of dataframe[<column_name>])`
* `drop(<column names>)`: select all except the given columns.
* `filter(<constraints>)`
* `where(<constraints>)`
* `withColumnRenamed(<existing_col_name>, <new_col_name>)`: rename columns.
* `withColumn(<column_name>, <column_expression>)`: add columns.
* `orderBy(<column>, ascending = True)`
* `sort(<column>, ascending = True)`
* `alias(<column_name>)`: name columns.

## SQL functions

### Scalar functions

Return a single value for each row based on calculations on one or more columns. 

* Math: `abs`, `log`, `avg`, `count`, etc.
* String: `length`, `concat`, `trim`, `contains`, etc.
* Time: `year`, `date_add`, `datediff`, `next_day`, etc.

### Aggregate functions

Return a single value for a group of rows. Can be used in combination with `groupBy()`.

* `df.groupBy().agg(<column_name>)`

### Window functions

Return several values for a group of rows. 

### User-defined functions

```python
from pyspark.sql.functions import *

udf_name = udf(lambda function definition)

data_frame.select(udf_name(col))
```

### Grouping Data

`.groupBy()`

* Take column names or a list of column objects.
* Return GroupedData Object.
* Can use an aggregate function or **`.agg(list_of_aggregate_fuctions)`**
  * `.groupBy(col).max(col)`
  * **`.groupBy(col).agg(max(col), min(col),...)`** 

### Joining Data

`df1.join(df2, condition or column name, join_type)`

* `join_type`:
  * `inner`(default)
  * `left_outer`
  * `right_outer`
  * `outer`: return unmatched rows from both tables.
  * `leftsemi`: get the names within left table that appear in right table, only contain the columns in left table, don't include the columns in right table.