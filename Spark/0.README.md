# 4.3. Spark

Extends the MapReduce model with primitives for efficient data sharing (Using Resilient Distributed Datasets (RDDs)).

- You can write distributed programs in a manner similar to writing local programs. 
- Spark’s collections abstract away the fact that they’re potentially referencing data distributed on a large number of nodes. 
- Spark combines MapReduce like capabilities for batch programing, real time data processing, SQL-like handling of structured data, graph algorithms and machine learning all in a single frame work. 

## Achieves

- Speed

  Run programs up to 100x faster than Hadoop MapReduce in **memory**, or 10x faster on
  **disk**.

- Easy of Use

  Write applications quickly in Java, Scala, Python, R.

- Generality

  Combine SQL, streaming, and complex analytics.

- Runs everywhere

  Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.

## Spark Stack

- Cluster Managers (Standalone/YARN/Mesos)
- Spark Core
- Spark SQL/Streaming/MLlib and ML/GraphX

![image-20181130210622391](/Users/crystal/gitbook/image/spark/sparkComponent.png)

###Spart Cluster

A set of interconnected processes,  running in a distributed manner on different machines.  

Three different cluster manager:

- Spark standalone
- Hadoop YARN
- Mesos

###Spark Core

Contains Spark functionalities required for running jobs and needed by other components. 

Resilient Distributed Dataset (RDD) : Abstraction of a distributed collection of items with operations and transformation applicable to the dataset. 

Fundamental functions such as basic I/O functionalities, networking, security, scheduling and data shuffling. 

###Spark SQL

Provides functions for manipulating large sets of distributed, structured data using an SQL subset. 

Uses DataFrames and DataSets: Spark SQL transforms operations on DataFrames/DataSets to operations on RDDs. 

Data Sources include Hive, JSON, relational databases, NoSQL databases and Parquet files. 

###Spark MLlib and ML

Library of machine learning algorithms 

Include logistic regression, naïve Bayes, support vector machine, decision trees, random forests, linear regression and k-mean clustering. 

MLlib :RDD-based APIs. 

Spark ML : DataFrame-based APIs. 

###Spark Streaming

Ingest real-time streaming data from various sources including HDFS, Kafka, Flume, Twitter, ZeroMQ and custom ones. 

Represent streaming data using discretized streams (Dstreams), which periodically create RDDs containing the data that came in during the last time window. 

Can be combined with other Spark components (Spark Core, SparkML and Mllib, GraphX, Spark SQL) 

###Spark GraphX

Provide functions for building graphs, represented as graph RDDs : EdgeRDD and VertexRDD. 

Contain important algorithms of graph theory such as page rank, connected components, shortest paths, SVD++ . 

## Spark Architecture

###Spark Runtime Components

1. Client 

   * Starts the driver program. 

   - Prepares the classpath and all configuration options. 
   - Client process: spark-submit, pyspark, spark-shell scripts 

2. Driver 

   * Orchestrates and monitors executor of a Spark application.
     * Once it gets information from the Spark master of all the workers in the cluster and where they are, the driver program distributes Spark tasks to each worker’s executor. 
   * The driver also receives computed results from each executor’s tasks. 
   * Always one driver per application.

3. Executor

   * Executes Spark tasks with a configurable number of cores.
   * Stores and caches all data partitions in its memory. 

Physical placement of executor and driver processes depends on the cluster types and its configuration:

1. Cluster-deploy mode: driver and executor both are on the cluster.
2. Client-deploy mode: only executor on the cluster, driver is on the client.

### Spark Cluster Components

Cluster Manager 

* Monitors the worker nodes and reserves resources upon request by the master. 

Master 

* Accepts applications to be run. 
* Requests resources in the cluster and makes them available to the driver. 
* Depending on the mode, it acts as a resource manager and decides where and how many executors to launch, and on what Spark workers in the cluster. 

Worker 

* Upon receiving instructions from Spark Master, the Spark worker JVM launches executors on the worker on behalf of the Spark Driver. 
* Spark has to be installed on all nodes. 

###Program mode

- local mode:

  The local mode and local cluster mode, although the easiest and quickest methods of setting up Spark, are used mainly for testing purposes.

- cluster mode