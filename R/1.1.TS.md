# 1.1. Using R for time series

## First look at data

### Load data and check data

Under R, there is a `Time-Series` object type.

#### Load built-in time-series data

```R
data(AirPassengers)
AirPassengers
```

![timeSeriesObjectType](/Users/crystal/dataScientistNotes/image/R/TS-tsobject.png)

`AirPassenger` is a `Time-Series` data. 

#### Convert data into time series data

```R
elec <- read.csv('/Users/crystal/xsun45/data/604/electronics.csv', header = F)
elec <- ts(elec, start = 1996, frequency = 12)
```

### Time series plot

```R
plot(AirPassengers)
# add some vertical line to check it seasonality
abline(v = seq(1949, 1960, 1), col = "red", lty = 2)
```

![image-20181210164042998](/Users/crystal/dataScientistNotes/image/R/TS-tsplot.png)

### Imputation

#### Check NA values

```R
statsNA(tsAirgap)
# output
[1] "Length of time series:"
[1] 144
[1] "-------------------------"
[1] "Number of Missing Values:"
[1] 13
[1] "-------------------------"
[1] "Percentage of Missing Values:"
[1] "9.03%"
[1] "-------------------------"
[1] "Stats for Bins"
[1] "  Bin 1 (36 values from 1 to 36) :      4 NAs (11.1%)"
[1] "  Bin 2 (36 values from 37 to 72) :      1 NAs (2.78%)"
[1] "  Bin 3 (36 values from 73 to 108) :      5 NAs (13.9%)"
[1] "  Bin 4 (36 values from 109 to 144) :      3 NAs (8.33%)"
[1] "-------------------------"
[1] "Longest NA gap (series of consecutive NAs)"
[1] "3 in a row"
[1] "-------------------------"
[1] "Most frequent gap size (series of consecutive NA series)"
[1] "1 NA in a row (occuring 10 times)"
[1] "-------------------------"
[1] "Gap size accounting for most NAs"
[1] "1 NA in a row (occuring 10 times, making up for overall 10 NAs)"
[1] "-------------------------"
[1] "Overview NA series"
[1] "  1 NA in a row: 10 times"
[1] "  3 NA in a row: 1 times"
```

#### Imputation

```R
# Random Imputation
na.random(tsAirgap)

# Mean Imputation
na.mean(tsAirgap, option = "mean")

# Median Imputation
na.mean(tsAirgap, option = "median")

# Mode Imputation
na.mean(tsAirgap, option = "mode")

# Last Observartion Carried Forward
na.locf(tsAirgap, option = "locf")

# Next Observartion Carried Backward
na.locf(tsAirgap, option = "nocb")

# Linear Interpolation
na.interpolation(tsAirgap, option = "linear")

# Spline Interpolation
na.interpolation(tsAirgap, option = "spline")

# Seasonal Adjustment then Random
na.seadec(tsAirgap, algorithm = "random")

# Seasonal Adjustment then Mean
na.seadec(tsAirgap, algorithm = "mean")

# Seasonal Adjustment then LOCF
na.seadec(tsAirgap, algorithm = "locf")

# Seasonal Adjustment then Linear Interpolation
na.seadec(tsAirgap, algorithm = "interpolation")
```

#### Check the imputation

```R
plot(na.seadec(tsAirgap, algorithm = "interpolation") - AirPassengers, ylim = c(-mean(AirPassengers), mean(AirPassengers)), ylab = "Difference", main = "Seas-Adj -> Linear")
mean((na.seadec(tsAirgap, algorithm = "interpolation") - AirPassengers)^2)
# output
[1] 3.701541
```

![image-20181203202915627](/Users/crystal/dataScientistNotes/image/R/TS-imputation.png)

## Modeling Data

### Check data

####Check for non-constant variance and apply a transformation

```R
# log-transformation
log(AirPassengers)

# Box-Cox transformation
opt.lambda <- BoxCox.lambda(br_train)
BoxCox(br_train, lambda = opt.lambda)
```

#### Check for seasonality and trend

####Check the Correlation between variables

```R
ccf(Bev, Food)
ccf(Bev, Indust)
ccf(Food, Indust)
```

![image-20181203202915627](/Users/crystal/dataScientistNotes/image/R/TS-ccf.png)



###Classical Decomposition Modeling

####Prepare

```R
# Extracting time as the explanatory variate from the time series framework of data
t <- time(AirPassengers)

# Introducing month as the season
cycle(AirPassengers)

# Introducing month as the season
month <- as.factor(cycle(AirPassengers))
```

####Model the trend-only

#####Linear trend

```R
reg0 <- lm(log(AirPassengers)~t)
summary(reg0)
plot(log(AirPassengers))
points(t,predict.lm(reg0),type='l',col='red') # superimpose the fit of model reg0 on the plot of the data
```

![image-20181210164338515](/Users/crystal/dataScientistNotes/image/R/TS-linearTrend.png)

#####p-order trend

```R
p <- 14
reg0 <- lm(as.numeric(elec) ~ poly(as.numeric(t), degree = p))
summary(reg0)

plot(elec, main = "Number of electronics orders in the Eurozone", ylab = "Number of elecronics orders", xlab = "Year")
points(t,predict.lm(reg0),type='l',col='red')
legend("topleft", 
  legend = c("Observed", "Trend"), 
  col = c("black", "red"),
  lty = c(1, 1))
```

####Model the seasonal-only

```R
reg1 <- lm(log(AirPassengers)~month)          # Just model seasonal effect
summary(reg1)
plot(log(AirPassengers))
points(t,predict.lm(reg1),type='l',col='red') # superimpose the fit of model reg0 on the plot of the data
```

![image-20181210164426523](/Users/crystal/dataScientistNotes/image/R/TS-seasonal.png)

####Model with trend and seasonal

```R
reg2 <- lm(log(AirPassengers)~t+month)        # Fitting the model reg2 with linear trend and seasonal effect
summary(reg2)
plot(log(AirPassengers))
points(t,predict.lm(reg2),type='l',col='red') # superimpose the fit of model reg2 on the plot of the data
```

![image-20181210164455386](/Users/crystal/dataScientistNotes/image/R/TS-trend+seasonal.png)

### Univariate - AR/MA/ARMA/ARIMA/SARIMA

#### Ordinary Differencing

```R
dchem <- diff(chem)

# check
acf(dchem, lag.max = 50)
ndiffs(x = dchem, test = "adf")
# output
[1] 1

adf.test(x = dchem, alternative = "stationary")
# output

	Augmented Dickey-Fuller Test

data:  AP1
Dickey-Fuller = -6.4313, Lag order = 5, p-value = 0.01
alternative hypothesis: stationary
```

#### Seasonal Differencing

```R
AP1.12 <- diff(AP1, lag = 12)
plot(AP1.12)

# check
acf(AP1.12, lag.max = 144)

nsdiffs(log(AirPassengers), m = 12)
# output
[1] 1
Warning message:
argument m is deprecated; please set the frequency in the ts object. 
```

#### Order selection

```
acf(BJ2)
pacf(BJ2)
```

![image-20181210164802790](/Users/crystal/dataScientistNotes/image/R/TS-armaOrder.png)

This figures suggest $p \leq 3, q \leq 1$.

#### Fitting models

##### AR models

```R
# Try fitting some of these models using ML
fit.ar <- arima(BJ2, order=c(3,0,0)) #AR(3)

# Now try fitting some of these models using LS
fitls.ar <- arima(BJ2, order=c(3,0,0), method="CSS") #AR(3)
```

##### MA models

```R
# Try fitting some of these models using ML
fit.ma <- arima(BJ2, order=c(0,0,1)) #MA(1)

# Now try fitting some of these models using LS
fitls.ma <- arima(BJ2, order=c(0,0,1), method="CSS") #MA(1)
```

##### ARMA models

```R
# Try fitting some of these models using ML
fit.arma <- arima(BJ2, order=c(3,0,1)) #ARMA(3,1)

# Now try fitting some of these models using LS
fitls.arma <- arima(BJ2, order=c(3,0,1), method="CSS") #ARMA(3,1)
```

##### ARIMA models

```R
# Try fitting some of these models using ML
fit.arima <- arima(x = chem, order = c(2,1,3)) #ARIMA(2,1,3)
```

##### SARIMA models

```R
fit.sarima <- arima(log(AirPassengers), order = c(3,1,1), seasonal = list(order = c(1,1,1), period = 12), method = "CSS-ML")
# fit.sarima <- Arima(AirPassengers, order = c(3,1,1), seasonal = list(order = c(1,1,1), period = 12), method = "CSS-ML", lambda = 0)
```

##### SARIMAX models

```R
# fit model with one covariate information
m2 <- arima(Bev, order = c(1,1,1), xreg = data.frame(Indust))
m3 <- arima(Bev, order = c(1,1,1), xreg = data.frame(Food))

# output
Call:
arima(x = Bev, order = c(1, 1, 1), xreg = data.frame(Indust))

Coefficients:
         ar1      ma1  Indust
      0.2951  -0.0820  0.4042
s.e.  0.1911   0.1932  0.0863

sigma^2 estimated as 32.12:  log likelihood = -791.61,  aic = 1591.23

# fit model with multi-covariate information
m4 <- arima(Bev, order = c(1,1,1), xreg = data.frame(Food, Indust))

# output
Call:
arima(x = Bev, order = c(1, 1, 1), xreg = data.frame(Food, Indust))

Coefficients:
         ar1      ma1    Food  Indust
      0.3113  -0.1069  0.4041  0.2493
s.e.  0.1890   0.1917  0.1166  0.0950

sigma^2 estimated as 30.66:  log likelihood = -785.75,  aic = 1581.5
```

##### Optimal model fitting

###### Manual

```R
#We can fit any number of models of different orders and use the output information to select 
#the model with the best fit. 
m1<-arima(BJ2,order=c(1,0,0))
m2<-arima(BJ2,order=c(2,0,0))
m3<-arima(BJ2,order=c(3,0,0))
m4<-arima(BJ2,order=c(4,0,0))
m5<-arima(BJ2,order=c(0,0,1))
m6<-arima(BJ2,order=c(0,0,2))
m7<-arima(BJ2,order=c(1,0,1))
m8<-arima(BJ2,order=c(2,0,1))
m9<-arima(BJ2,order=c(3,0,1))
m10<-arima(BJ2,order=c(4,0,1))
m11<-arima(BJ2,order=c(1,0,2))
m12<-arima(BJ2,order=c(2,0,2))
m13<-arima(BJ2,order=c(3,0,2))
m14<-arima(BJ2,order=c(4,0,2))

sigma2<-c(m1$sigma2,m2$sigma2,m3$sigma2,m4$sigma2,m5$sigma2,m6$sigma2,m7$sigma2,m8$sigma2,m9$sigma2,m10$sigma2,m11$sigma2,m12$sigma2,m13$sigma2,m14$sigma2)
loglik<-c(m1$loglik,m2$loglik,m3$loglik,m4$loglik,m5$loglik,m6$loglik,m7$loglik,m8$loglik,m9$loglik,m10$loglik,m11$loglik,m12$loglik,m13$loglik,m14$loglik)
AIC<-c(m1$aic,m2$aic,m3$aic,m4$aic,m5$aic,m6$aic,m7$aic,m8$aic,m9$aic,m10$aic,m11$aic,m12$aic,m13$aic,m14$aic)
d <- data.frame(pq = c("(1,0)","(2,0)","(3,0)","(4,0)","(0,1)","(0,2)","(1,1)","(2,1)","(3,1)","(4,1)","(1,2)","(2,2)","(3,2)","(4,2)"),sigma2,loglik,AIC)
d

# Order this by sigma^2
d[order(d$sigma2),]

# Order this by loglik
d[order(-d$loglik),]

# Order this by AIC
d[order(d$AIC),]
```

![image-20181210173121758](/Users/crystal/dataScientistNotes/image/R/TS-armaModels.png)

###### Loop

```R
p <- 0:5
d <- 0:1
q <- 0:4
P <- 0:2
D <- 0:1
Q <- 0:3
para <- as.matrix(expand.grid(p,d,q,P,D,Q))
acc <- matrix(1:nrow(para), ncol = 1) 

for(k in 1:nrow(para)){
  arimaFit = tryCatch(arima(log(beer), order=c(para[k,1], para[k,2], para[k,3]), 
                            seasonal=list(order=c(para[k,4], para[k,5], para[k,6]), 
                            period=12), method="ML"),
                      error=function( err ) FALSE,
                      warning=function( err ) FALSE )
  if(!is.logical(arimaFit)) {
      # selected by likelihood ratio test
      if (k==1) {
      m.red <- arimaFit
      } else {
        D <- -2*(m.red$loglik - arimaFit$loglik)
        pval <- 1-pchisq(D,2)
        if (pval < 0.05) {
          m.red <- arimaFit
        }
      }
      
      # selected by RMSE
      pred <- forecast(object = model, h = 72, level = 0.95, lambda = 0, biasadj = TRUE)
  	  acc[k,1] <- rmse(test, pred$mean)
  } else {
    next
  }
}

# selected by likelihood ratio test
summary(m.red)

# selected by RMSE
data.frame(cbind(para,acc)) %>% 
  arrange(V7) %>% 
  head(10) %>% 
  knitr::kable(col.names = c("p","d","q","P","D","Q","RMSE"),
               caption = "\\label{tab:tab}Top 10 model's parameter and RMSE",
               digits = 2)
```

###### Auto

```R
# use auto.arima to find optimal model
auto.arima(chem)
```

#### Visualize the model

```R
f <- chem - m$residuals #fitted values
plot(chem, type = "l", main = "Daily Chemical Concentrations", ylab = "Concentrations", xlab = "Days")
points(f, type = "l", col = "red")
# lines(exp(f), col = "red") # for transformed data
legend("bottomright", legend = c("Observed", "Predicted"), lty = 1, col = c("black", "red"), cex = 0.5)
```

![image-20181203202915627](/Users/crystal/dataScientistNotes/image/R/TS-arima.png)

### Univariate - Smoothing Exponential

```R
# Single (Simple) Exponential Smoothing
hw.LH <- HoltWinters(x = LakeHuron, beta = F, gamma = F)
hw.LH <- HoltWinters(x = LakeHuron, alpha = 0.2, beta = F, gamma = F)

# Double Exponential Smoothing
hw.CH <- HoltWinters(x = chem, gamma = F) 
hw.CH <- HoltWinters(x = chem, alpha = 0.2, beta = 0.7, gamma = F) 

# Triple Exponential Smoothing -- Additive
hw.AD <- HoltWinters(x = USAccDeaths,alpha = 0.2, beta =  0.2, gamma = 0.2, seasonal = "add") 

# Triple Exponential Smoothing -- Multiplicative
hw.AP <- HoltWinters(x = AirPassengers, seasonal = "mult") 
hw.AP <- HoltWinters(x = AirPassengers, alpha = 0.2, beta = 0.1, seasonal = "mult") 
```

###Multivariate - VAR model

#### Order Selection

```R
# select order p
VARselect(y = data.frame(Bev, Food, Indust))

# output
$selection
AIC(n)  HQ(n)  SC(n) FPE(n) 
     9      2      2      9 

$criteria
                 1           2           3           4           5           6           7
AIC(n)    8.837087    8.515259    8.543184    8.590641    8.629916    8.573736    8.564217
HQ(n)     8.906780    8.637221    8.717416    8.817142    8.908686    8.904777    8.947527
SC(n)     9.010092    8.818018    8.975697    9.152908    9.321936    9.395511    9.515746
FPE(n) 6884.970839 4990.581293 5132.384156 5382.733150 5599.849950 5296.003321 5248.655238
                 8           9          10
AIC(n)    8.527629    8.490873    8.533647
HQ(n)     8.963209    8.978722    9.073766
SC(n)     9.608912    9.701910    9.874437
FPE(n) 5063.678635 4885.350319 5104.578541
```

#### Fit model

```R
# fit model with selected order p
VAR(y = data.frame(Bev, Food, Indust), p = 1)

# output
VAR Estimation Results:
======================= 

Estimated coefficients for equation Bev: 
======================================== 
Call:
Bev = Bev.l1 + Food.l1 + Indust.l1 + const 

     Bev.l1     Food.l1   Indust.l1       const 
 0.94733504  0.04823936  0.01897155 -0.91891633 


Estimated coefficients for equation Food: 
========================================= 
Call:
Food = Bev.l1 + Food.l1 + Indust.l1 + const 

      Bev.l1      Food.l1    Indust.l1        const 
-0.005942198  0.942819303  0.049593166  2.050464222 


Estimated coefficients for equation Indust: 
=========================================== 
Call:
Indust = Bev.l1 + Food.l1 + Indust.l1 + const 

      Bev.l1      Food.l1    Indust.l1        const 
 0.003677795 -0.033880216  1.017185919  1.859971917 
```

##Choose a Model

### Goodness-of-fit



###Likelihood Ratio Test

```R
# D <- -2*(null.mod$loglik - alt.mod$loglik)
# pval <- 1-pchisq(D,n.alt-n.null)
# print(c("Test Statistic:",D,"P-value:",pval))

# Compare MA(1) with ARMA(1,2)
D <- -2*(m5$loglik - m11$loglik)
pval <- 1-pchisq(D,2)
print(c("Test Statistic:", round(D, 4), "P-value:", round(pval, 4)))

# output
[1] "Test Statistic:" "5.587"           "P-value:"        "0.0612"   
```

### RMSE

```R
pred <- forecast(object = model, h = 72, level = 0.95, lambda = 0, biasadj = TRUE)
acc[k,1] <- rmse(test, pred$mean)
```

###Diagnostic plots

```R
# plot of fitted values vs residuals
plot(reg2$fitted, reg2$residuals, main = "Residuals vs. Fitted Values", ylab = "Residuals", xlab = "Fitted Values")
abline(h = 0, col = "red", lwd = 2)

#qq-plot of residuals
qqnorm(reg2$residuals)
qqline(reg2$residuals, col = "red")	# plotting the line, along which the dots in qq-plot should lie

# plotting the residuals vs time
plot(reg2$residuals, main = "Residuals vs. Time", ylab = "Residuals", xlab = "Time")
abline(h = c(-3,3), col = "red", lty = 2)
abline(h = 0, col = "red", lwd = 2)    # plotting a horizontal line at 0

# sample acf plot of residuals
acf(reg2$residuals, main = "ACF Plot of Residuals")
```

![image-20181210164604950](/Users/crystal/dataScientistNotes/image/R/TS-diagnostic.png)

The **residuals vs. fitted values** plot indicates a fairly **random** scattering of points; there are no obvious "**funnel-shaped**" trends suggesting that the **error variance is indeed constant**.

The **QQ-plot** indicates that the residual quantiles match fairly closely with the theoretical Normal quantiles (**straight line**), suggesting that the **Normality assumption** is reasonable.

The **residuals vs. time** oder also suggests that the error varianes \( $$\sigma^2$$\) does not depend much on time, suggesting that the **constant variance assumption** is reasonable.

The **ACF plot** indicates significant autocorrelation at lags 1 to 14, suggesting that residuals separated in time by 1 to 12 time points are significantly correlated, which contradicts the assumption that the **errors are independent**.

### Hypothesis test

```R
# test norality
shapiro.test(m$residuals)
# output

	Shapiro-Wilk normality test

data:  m$residuals
W = 0.98578, p-value = 0.1444

```



##Prediction

###Predict with class model

```R
t.new <- seq(1961,1963,length=25)[1:24]  # Intoducing new time for forecatsting 2 years 1961 and 1962 (notice that it is 1:24 because 1963 should not be included)
t2.new <- t.new^2
month.new <- factor(rep(1:12,2))         # Introducing the seasonal value for forecasting

new <- data.frame(t=t.new, t2=t2.new, month=month.new) # Putting the values for forecasting into a dataframe
pred <- predict.lm(reg4,new,interval='prediction')     # Computing the prediction as well as prediction interval

par(mfrow=c(1,1))
plot(AirPassengers,xlim=c(1949,1963),ylim=c(0,900))    #plotting the data

abline(v=1961,col='blue',lty=2)          # adding a vertical line at the point where prediction starts
lines(exp(pred[,1])~t.new,type='l',col='red')          # plotting the predict
lines(exp(pred[,2])~t.new,col='green')   # plotting lower limit of the prediction interval
lines(exp(pred[,3])~t.new,col='green')   # plotting upper limit of the  prediction interval
```

![image-20181210164802790](/Users/crystal/dataScientistNotes/image/R/TS-predict.png)

### Predict with AR/MA/ARMA/ARIMA/SARIMA model

```R
plot(forecast(object = m, h = 14, level = 0.95))
# transformed data
pred <- forecast(object = model.optimal, h = 72, level = 95, lambda = 0, biasadj = TRUE)
```

![image-20181210164802790](/Users/crystal/dataScientistNotes/image/R/TS-arimaForecast.png)

###Predict with Holt-Winters

```R
plot(forecast(hw.LH, h = 5)) #Forecast is a constant. SES not appropriate for a time series with trend.
plot(forecast(HoltWinters(x = AirPassengers, beta = F, gamma = F), h = 60)) #Single
plot(forecast(HoltWinters(x = AirPassengers, gamma = F), h = 60))           #Double
plot(forecast(HoltWinters(x = AirPassengers, seasonal = "add"), h = 60))    #Triple -- Additiive
plot(forecast(HoltWinters(x = AirPassengers, seasonal = "mult"), h = 60))   #Triple -- Multiplicative
```

###Prediction with SARIMAX

```R
f.arimax <- forecast(m4, h = 3, xreg = data.frame(Food = test$FoodInd, Indust = test$IndustInd))
plot(f.arimax)
(rmse.arimax <- sqrt(mean((f.arimax$mean - test$BevInd)^2)))

# output
[1] 7.537393
```

![image-20181203202915627](/Users/crystal/dataScientistNotes/image/R/TS-sarimaxPredict.png)

###Predict with VAR

```R
pred <- predict(m.var, n.ahead = 3, ci = 0.95)
plot(pred)
rmse.var <- sqrt(mean((pred$fcst$Bev[,1] - test$BevInd)^2))

# output
[1] 2.778531
```

![image-20181203202915627](/Users/crystal/dataScientistNotes/image/R/TS-var.png)



## Sample ACF/PACF plots

#### Sample ACF for IID noise N\(0,1\)

```text
X <- rnorm(100)                   # generating (independently) 100 realizations of N(0,1)
par(mfrow=c(2,1))                 #dividing the page into 2 rows and one column
plot(X,type='l',main='iid noise') #plotting the data
acf(X,main='Sample ACF for iid noise', lag.max = 100) # plotting the acf
```

![image-20181210164802790](/Users/crystal/dataScientistNotes/image/R/TS-iidNoise.png)

#### Sample ACF for MA\(1\) process

```R
# simulating data from an MA(1) process
data.sim <- arima.sim(n = 200, list(ma = c(0.7)), sd = sqrt(1))
plot(data.sim, main="Simulated Data from an MA(1) Process") # plotting the data
acf(data.sim, main = "") # plotting the acf
pacf(data.sim, main = "") # plotting the pacf
# acf(data.sim, type="partial")
```

![image-20181210164802790](/Users/crystal/dataScientistNotes/image/R/TS-ma1.png)

#### Sample ACF for AR\(1\) process

```R
# simulating data from an AR(1) process
data.sim <- arima.sim(n = 200, list(ar = c(0.7)), sd = sqrt(1))
plot(data.sim, main="Simulated Data from an AR(1) Process") #plotting the data
acf(data.sim, main = "") # plotting the acf
pacf(data.sim, main = "") # plotting the pacf
```

![image-20181210164802790](/Users/crystal/dataScientistNotes/image/R/TS-ar1.png)

####Sample ACF for ARMA(1,1) process

```
data.sim <- arima.sim(n =10000, list(ar=c(0.5) , ma = c(1)), sd = sqrt(1))
plot(data.sim, main="Simulated Data from an ARMA(1,1) Process")
acf(data.sim, main = "")
pacf(data.sim, main = "")
```

![image-20181210164802790](/Users/crystal/dataScientistNotes/image/R/TS-arma11.png)

#### Sample ACF for data with trend

```text
a <- seq(1,100,length=200)
X <- 22-15*a+0.3*a^2+rnorm(200,500,50)
par(mfrow=c(2,1)) #dividing the page into 2 rows and one column
ts.plot(X, main = "Time Series With Significant Trend")
acf(X, main = "ACF Exhibits Seasonality + Slow Decay", lag.max = 60)
```

![image-20181210164802790](/Users/crystal/dataScientistNotes/image/R/TS-acfTrend.png)

#### Sample ACF for data with seasonality

```text
X <- 100*sin(1:200)+rnorm(200, 0, 0.5) 
par(mfrow=c(2,1))
ts.plot(X, main = "Time Series With Significant Seasonality")
acf(X, main = "ACF Also Exhibits Seasonality", lag.max=60)
```

![image-20181210164802790](/Users/crystal/dataScientistNotes/image/R/TS-acfSeasonal.png)

#### Sample ACF for data with trend and seasonal component

```text
a <- seq(1,10,length=200)
X <- 22-15*a+a^2+5*sin(20*a)+rnorm(200,20,2)
par(mfrow=c(2,1)) #dividing the page into 2 rows and one column
ts.plot(X, main = "Time Series With Significant Trend and Seasonality")
acf(X, main = "ACF Exhibits Seasonality + Slow Decay", lag.max = 60)
```

![image-20181210164802790](/Users/crystal/dataScientistNotes/image/R/TS-acfTrendSeasonal.png)
