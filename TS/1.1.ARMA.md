# 1.1. Univariance - ARMA models

<<<<<<< HEAD
## Mathematical Prerequistites

###Back Shift Operator - B

$$
\begin{aligned}
Bf(t) &= f(t-1)\\
BY_t &= Y_{t-1}\\
B^2Y_t &= BBY_t = BY_{t-1} = Y_{t-2}\\
B^nY_t &= Y_{t-n} \; \text{for}\; n=0, 1, 2, ...\\
B^0 &= 1
\end{aligned}
$$

### Power Series

### Complex Number



## AR(p) Process

The time series {$Y_t$} is called an autoregression process of order $p$ if $Y_t=\phi_1 Y_{t-1} + \phi_2 Y_{t-2} + ... + \phi_p Y_{t-p} + \varepsilon_t$, where $\{ \varepsilon_t \} \sim WN(0, \sigma^2)$ and $\phi_1, \phi_2, ..., \phi_p$ are constants.
$$
\begin{aligned}
Y_t &= \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + ... + \phi_p Y_{t-p} + \varepsilon_t \\
Y_t - \phi_1 Y_{t-1} - \phi_2 Y_{t-2} - ... - \phi_p Y_{t-p} &= \varepsilon_t \\
B^0Y_t - \phi_1 B^1Y_t - \phi_2 B^2Y_t - ... - \phi_p B^pY_t &= \varepsilon_t \\
(B^0 - \phi_1 B^1 - \phi_2 B^2 - ... - \phi_p B^p)Y_t &= \varepsilon_t \\
(1-\sum_{i=1}^p \phi_iB^i)Y_t &= \varepsilon_t \\
\phi^p(B)Y_t &= \varepsilon_t
\end{aligned}
$$
Where $\phi^p(Z) = 1 - \sum_{i=1}^p \phi_i Z^i$ is the generating function of the AR(p) process.

###Stationary

An AR(p) process is only stationary if the stationary condition on the $\phi$'s is met:
$$
\begin{aligned}
\phi^p(B)Y_t &= \varepsilon_t \\
Y_t &= \frac{1}{\phi^p(B)} \varepsilon_t \\
\text{where}\; \frac{1}{\phi^p(B)} &= \sum_{n=0}^\infty \psi_n B^n \\
&= \psi_0 + \psi_1B + \psi_2B^2 + ... \\
&= \psi(B) \\
Y_t &= \psi(B) \varepsilon_t \\
&= (\psi_0 + \psi_1B + \psi_2B^2 + ...) \varepsilon_t \\
&= \psi_0 \varepsilon_t + \psi_1 \varepsilon_{t-1} + \psi_2 \varepsilon_{t-2} + ...
\end{aligned}
$$
In order for this to be useful, we require that $\psi_0 \varepsilon_t + \psi_1 \varepsilon_{t-1} + \psi_2 \varepsilon_{t-2} + ...$ converges. The converges if $\sum_{n=0}^\infty \psi_n B^n$ converges, which happens if and only if the zeros (roots) of $\phi(Z)$ lie inside the unit circle in the complex plane.

Thus an AR(p) can be written as an MA($\infty​$) model if $\phi(Z)​$, the AR generating function, has zeros outside that unit circle in the complex plane:
$$
\phi(Z) \neq 0 \;\text{for any}\; Z \;\text{such that}\; |Z| \leq 1 
\Leftrightarrow \phi(Z) = 0 \;\text{only for}\; Z \;\text{such that}\; |Z| > 1
$$

###Remarks

* An AR(p) process has a PACF which satisfies $\alpha(h) = \begin{cases} \neq0 \; \text{if} \; h \leq p \\ =0  \; \text{if} \; h > p \end{cases}​$.
* A time series has a PACF plot that exhibits $q​$ initial spikes, then negligibleibly small spikes thereafter can be modeled by an AR(p) model.
* AR(p) = MA($\infty$), ACF of AR(p) process shows exponential decay between the ACF for MA(q) shut off for $h \geq q$, but here $q = \infty$ and so AR(p) does not shuts off on an ACF.

## MA(q) Process

The time series {$Y_t$} is called a moving average process of order $q$ if $Y_t = \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + ... + \theta_q \varepsilon_{t-q} $, where $\{ \varepsilon_t \} \sim WN(0, \sigma^2)$ and $\theta_1, \theta_2, ..., \theta_q$ are constants.
$$
\begin{aligned}
Y_t &= \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + ... + \theta_q \varepsilon_{t-q} \\
Y_t &= B^0\varepsilon_t + \theta_1 B^1\varepsilon_t + \theta_2 B^2\varepsilon_t + ... + \theta_q B^q\varepsilon_t \\
Y_t &= (B^0 + \theta_1 B^1 + \theta_2 B^2 + ... + \theta_p B^p)\varepsilon_t \\
Y_t &= (1+\sum_{i=1}^q \theta_iB^i)\varepsilon_t \\
Y_t &= \theta^q(B)\varepsilon_t
\end{aligned}
$$
Where $\theta^q(Z) = 1 + \sum_{i=1}^q \theta_i Z^i$ is the generating function of the MA(q) process.

###Invertible

$MA(q) = AR(\infty)$ if the invertibility condition on the $\theta$'s is met:
$$
\begin{aligned}
Y_t &= \theta^q(B)\varepsilon_t \\
\frac{1}{\theta^q(B)} Y_t &= \varepsilon_t \\
\text{where}\; \frac{1}{\theta^q(B)} &= \sum_{n=0}^\infty \lambda_n B^n \\
&= \lambda_0 + \lambda_1B + \lambda_2B^2 + ... \\
&= \lambda(B) \\
\lambda(B) Y_t &= \varepsilon_t \\
(\lambda_0 + \lambda_1B + \lambda_2B^2 + ...) &= \varepsilon_t \\
\lambda_0 Y_t + \lambda_1 Y_{t-1} + \lambda_2 Y_{t-2} + ... &= \varepsilon_t
\end{aligned}
$$
In order for this to be useful, we require that $\lambda_0 Y_t + \lambda_1 Y_{t-1} + \lambda_2 Y_{t-2} + ...$ converges. The converges if $\sum_{n=0}^\infty \lambda_n B^n$ converges, which happens if and only if the zeros (roots) of $\theta(Z)$ lie inside the unit circle in the complex plane.

Thus an MA(q) can be written as an AR($\infty$) model if $\theta(Z)$, the MA generating function, has zeros outside that unit circle in the complex plane:
$$
\theta(Z) \neq 0 \;\text{for any}\; Z \;\text{such that}\; |Z| \leq 1
\Leftrightarrow \theta(Z) = 0 \;\text{only for}\; Z \;\text{such that}\; |Z| > 1
$$

####Examples:

$$
\begin{aligned}
\text{Prove:} \; MA(1) &= AR(\infty) \\
Y_t &= \varepsilon_t + \theta \varepsilon_{t-1} \Rightarrow \varepsilon_t = Y_t - \theta \varepsilon_{t-1} \\
&= \varepsilon_t + \theta (Y_{t-1} - \theta \varepsilon_{t-2}) \\
&= \varepsilon_t + \theta Y_{t-1} - \theta^2 \varepsilon_{t-2} \\
&= \varepsilon_t + \theta Y_{t-1} - \theta^2 (Y_{t-2} - \theta \varepsilon_{t-3}) \\
&= \varepsilon_t + \theta Y_{t-1} - \theta^2 Y_{t-2} + \theta^3 \varepsilon_{t-3} \\
&= ...
\end{aligned}
$$

###Remarks

* An MA(q) process is always stationary regardless of $q$ and the values of the $\theta$'s.
* An MA(q) process is "$q$-correlated", which means that $\rho(h) = \begin{cases} \neq0 \; \text{if} \; h \leq q \\ =0  \; \text{if} \; h > q \end{cases}$.
* A time series with an ACF plot that exhibits $q$ initial spikes, then negligibleibly small spikes thereafter can be modeled by an MA(q) model.
* MA(q) = AR($\infty$), PACF of MA(q) process shows exponential decay between the PACF for AR(p) shut off for $h \geq p $, but here $p = \infty$ and so MA(q) does not shuts off on an PACF.

## ARMA(p, q) models

The time series {$Y_t$} is called an autoregression moving averages of order $p$ and $q$ if:

$Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + ... + \phi_p Y_{t-p} +\varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + ... + \theta_q \varepsilon_{t-q} ​$, where $\{ \varepsilon_t \} \sim WN(0, \sigma^2)​$ and $\phi​$'s and $\theta​$'s are constants.
$$
\begin{aligned}
Y_t &= \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + ... + \phi_p Y_{t-p} +\varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + ... + \theta_q \varepsilon_{t-q} \\
Y_t - \phi_1 Y_{t-1} - \phi_2 Y_{t-2} - ... - \phi_p Y_{t-p} &= \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + ... + \theta_q \varepsilon_{t-q} \\
Y_t - \phi_1 B Y_t - \phi_2 B^2 Y_t - ... - \phi_p B^p Y_t &= B^0\varepsilon_t + \theta_1 B^1\varepsilon_t + \theta_2 B^2\varepsilon_t + ... + \theta_q B^q\varepsilon_t \\
(1 - \phi_1 B - \phi_2 B^2 - ... - \phi_p B^p)Y_t &= (B^0 + \theta_1 B^1 + \theta_2 B^2 + ... + \theta_p B^p)\varepsilon_t \\
\phi^p(B)Y_t &= \theta^q(B)\varepsilon_t
\end{aligned}
$$
Where $\phi^p(Z) = 1-\phi_1Z-\phi_2Z^2-...-\phi_pZ^p$ is the generating function for the AR component, $\theta^q(Z) = 1+\theta_1Z+\theta_2Z^2+...+\theta_qZ^q$ is the generating function for the MA component.

### Remarks

- An ARMA model is not necessary stationary, but we'd like it to be so that we can model stationary time series.
- An ARMA model is not necessary invertible, but we'd like it to be so that $Y_t$ can be written exclusively as a function of its sum history.
- An ARMA(p, q) model is stationary if and only if its AR component is stationary. We check this by determining the AR generating function $\phi(Z)$ satisfies the stationary condition.
- An ARMA(p, q) model is invertible if and only if its MA component is invertible. We check this by determining the MA generating function $\theta (Z)$ satisfies the invertibility condition.
- AR(p) = ARMA(p, 0).
- MA(q) = ARMA(0, q).
- Model selection is determined by selecting appropriate orders $p$ and $q$. We can use ACF and PACF plots to help with this what we expect to see on these plots is the following:
  - ACF: $q$ significance spikes ($h \leq q$) + exponential decay
  - PACF: $p$ significance spikes ($h \leq p$) + exponential decay

### Estimating

Using observed data we want to estimate all of the parameters $\phi_1, \phi_2, \phi_3, ..., \phi_p, \theta_1, \theta_2, \theta_3, ..., \theta_q, \sigma$.

#### ML estimation

To do this we need to make assumption about distribution of the errors, and hence the data:
$$
\begin{aligned}
\vec{\varepsilon} &=
  \begin{bmatrix}
  \varepsilon_{1} \\
  \varepsilon_{2} \\
  \vdots \\
  \varepsilon_{n}
  \end{bmatrix} \sim MVN(\vec{0}, \sigma^2I) \\
\Rightarrow \vec{Y} &=
  \begin{bmatrix}
  Y_{1} \\
  Y_{2} \\
  \vdots \\
  Y_{n}
  \end{bmatrix} \sim MVN(\vec{0}, \Gamma) \\
\text{where}\; \Gamma &=
  \begin{bmatrix}
  \gamma(0) & \gamma(1) & \gamma(2) & \dots & \gamma(n-1) \\
  \gamma(1) & \gamma(0) & \gamma(1) & \dots & \gamma(n-2) \\
  \gamma(2) & \gamma(1) & \gamma(0) & \dots & \gamma(n-3) \\
  \vdots & \vdots & \vdots & & \vdots  \\
  \gamma(n-1) & \gamma(n-2) & \gamma(n-3) & \dots & \gamma(0) \\
  \end{bmatrix}
\end{aligned}
$$
With distributional assumption we can write the likelihood function as:
$$
L(\phi_1, \phi_2, ..., \phi_p, \theta_1, \theta_2, ..., \theta_q, \sigma) = \frac{1}{(2\pi)^{\frac{n}{2}} |\Gamma|^{\frac{1}{2}}} e^{-\frac{1}{2} \vec{y}^T \Gamma^{-1} y}
$$
where $\vec{y} = (y_1, y_2, ..., y_n)^T$.

We want to maximize this function and specifically determine the parameter values at which the maximum is attained.

Once a model has been fit to data, we should verify that if follows the assumptions we assume that it does specifically we assume the $\varepsilon_t \sim WN(0, \sigma^2)$. We can think of residuals $\{e_t\}$ as sample observations of $\{\varepsilon_t \}$ and so we expect the residuals to behave in the same way. Specifically we check whether $\{e_t\}$:

- have zero-mean
- have constant variance
- are uncorrelated

* follow a normal distribution

####LS estimation

We want to find values of $\phi_1, \phi_2, \phi_3, ..., \phi_p, \theta_1, \theta_2, \theta_3, ..., \theta_q, \sigma$ that minimize error specifically sum of squared error:
$$
S(\phi_1, \phi_2, ..., \phi_p, \theta_1, \theta_2, ..., \theta_1) = \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$
Notice, this is not a function of $\sigma$, we estimate $\sigma$ after having estimated the $\phi$'s and $\theta$'s to be:
$$
\hat{\sigma} = \sqrt{\frac{S(\hat{\phi}, \hat{\theta})}{n-p-q}}
$$
Once a model has been fit to data, we should verify that if follows the assumptions we assume that it does specifically we assume the $\varepsilon_t \sim WN(0, \sigma^2)$. We can think of residuals $\{e_t\}$ as sample observations of $\{\varepsilon_t \}$ and so we expect the residuals to behave in the same way. Specifically we check whether $\{e_t\}$:

* have zero-mean
* have constant variance
* are uncorrelated

#### Order Selection

We use $\hat{\sigma}^2$ (smaller), $logL(\hat{\phi}, \hat{\theta}, \hat{\sigma})$ (bigger) and AIC (smaller) to help choose an optimal model (i.e. the best $p$ and $q$).

##### AIC

* AIC includes a penalty for having an overly complicated model and hence protects us from over-fitting. The other metrics don't do this.

* We cannot calculate these information criteria if we do LS estimation.

$$
AIC = -2logL(\hat{\phi}, \hat{\theta}, \hat{\sigma}) + 2(p+q+1) \\
AICC = -2logL(\hat{\phi}, \hat{\theta}, \hat{\sigma}) + \frac{2(p+q+1)n}{n-p-q-2}
$$

#####Likelihood ratio model

We can formally compare nested model using a likelihood ratio model L(RT)

$H_0​$: reduced and full models fit this data equally well.

$H_a$: full model fits the data better than the reduced data.
$$
D = -2log\bigg[\frac{L(\text{reduced model})}{L(\text{full model})}\bigg] \sim \chi^2_{(M_F-M_R)}
$$
where $M_F$ is equally to the number of parameters in full model, $M_R$ is equally to the number of parameters in reduced model.

=======
>>>>>>> origin/sfcta
