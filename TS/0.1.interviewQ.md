# 0.1. Interview Questions

## Time Series Overview

### Briefly describe what is meant by *trend* in a time series context.

The trend in a time series is the longer-term consistent increases and/or decreases. Trend, in a sense, is the smoothed general "shape" of the time series.

###Briefly describe waht is meant by *seasonality* in a time series context.

Seasonality refers to regular and predictable fluctuations that repeat indefinitely according to some period.

#### Using the autocorrelation function (ACF) plot shown below, determine the period s of this time series and briefly justify your choice. [2018Quiz1]

####diagnostic [2018Quiz1/2016Quiz1/2018Quiz4]

## Stationary

### Define what it means for a time series {$Y_t$} to be *strictly* stationary.

{$Y_t$} is a strictly stationary time series if the joint distribution of $Y_{t_1}, Y_{t_2},...,Y_{t_n}$ is the same as $Y_{t_{1+h}}, Y_{t_{2+h}},...,Y_{t_{n+h}}$ for all $t_1,..., t_n, h \in Z$. Thus, the joint distribution is preserved under time shifts.

###A time series {$Y_t$} is said be *weakly* stationary if it satisfies two properties.

* $E(Y_t) = \mu$ is independent of $t$.
* $Cov(Y_t, Y_{t+h}) = \gamma(h)$ is independent of $t$ for all $h$.
* $E(X_t^2) < \infty$ for all $t$.

####Every white noise process is an IID noise process? [2018Quiz2]

#### ${\varepsilon_t} \sim IID(0, \sigma^2)$ [2016Quiz1] 

####${\varepsilon_t} \sim WN(0, \sigma^2)$ [2015Quiz1]

## ARMA Process

### Explain the difference between an *autocorrelation function* an a *partial autocorrelation function*.

The autocorrelation of lag $h$ calculates the correlation between observations $h$ time units apart.

The partial autocorrelation function of lag $h$ also calculates the correlation between observations $h$ lags apart, but it does so after having accounted for any possible relationships with observations at intermediate lags. What results is a quantification of the linear relationship at two time points, corrected for associations of intermediate time points.

###For a general $MA(q)$ process, state the property that is helpful for identifying the order $q$.

The ACF of an $MA(q)$ process satisfies the following property:

* $\rho(h) \neq 0$ if $h \leq q$
* $\rho(h) = 0$ if $h > q$

###For a general $AR(p)$ process, state the property that is helpful for identifying the order $p$.

The PACF of an $AR(p)$ process satisfies the following property:

* $\alpha(h) \neq 0$ if $h \leq p$
* $\alpha(h) = 0$ if $h > p$

###For each of the following processes, describe what you would expect to see on an ACF plot, and on a PACF plot.

####$AR(p)$

* ACF: (mixed) exponential decay.
* PACF: $p$ initial spikes, then negligibly small spikes thereafter.

#### $MA(q)$

* ACF: $q$ initial spikes, then negligibly small spikes thereafter.
* PACF: (mixed) exponential decay.

####$ARMA(p,q)$

* ACF: $q$ initial spikes and exponential decay.
* PACF: $p$ initial spikes and exponential decay.

###Below are the ACF and PACF of three particular time series, what is your best guess as to the nature of the time series? [2015Quiz2]

###Estimation

####Explain the intuition behind the Maximum Likelihood (ML) approach.

The idea with maximum likelihood estimation is to find the parameter values that maximize the likelihood function. In other words, we want to find the parameter values most likely to have generated the data that we actually observed.

####Explain the intuition behind the Least Squares (LS) approach.

The idea with least squares estimation is to find the parameter values that minimize the error sum of squares. In other words we want to find the parameter values that make the fitted values as close as possible to the observed values.

####Why might you prefer the least squares approach over maximum likelihood approach.

* Maximum likelihood estimation requires making a distributional assumption whereas Least squares estimation does not.
* Minimzing the LS objective function is typically less computationally intenserive than maximizing the (log) likelihood function.

####Why might you prefer the maximum likelihood approach over least squares approach.

- Under ML estimation the parameter estimators follow normal distributions (asymptotically) that are unbiased and efficient.
- We can obtain confidence intervals and hypothesis tests that are based on normal distributions and not heristics.
- We get a richer collection of likelihood-based goodness-of-fit metrics.

#### State the $WN(0, \sigma^2)$ assumptions that must be validated.

* $\varepsilon_t$ should have expectation 0 for all $t$.
* $\varepsilon_t$ should exhibit constant variance for all $t$.
* $\varepsilon_t$ and $\varepsilon_s$ should be uncorrelated for all $t \neq s$.

## ARIMA & SARIMA

### ARIMA

####Describe the type of non-stationary time series for which an ARIMA model would be appropriate.

An ARIMA model is typically used when a non-stationary time series exhibits "trend" (but not "seasonality").

####Explain the role of "differencing" in the context of an ARIMA model.

In this case we, can lag 1 difference ($\nabla Y_t = Y_t - Y_{t-1}$) the time series finite many ($d$) times to eliminate trend.

####Explain the orders $p$, $d$, and $q$ in the context of an ARIMA(p,d,q) model.

The resulting stationary time series can then be modeled by an $ARMA(p,q)$ model with autoregressive and moving average orders respectively given by $p$ and $q$.

###SARIMA

####Describe the type of non-stationary time series for which an SARIMA model would be appropriate.

An SARIMA model is typically used when a non-stationary time series exhibits "seasonality" alone or "trend" and "seasonality" together.

####Explain the role of "ordinary" and "seasonal" differencing in this context.

In this case seasonal (lag $m$, where $m$ is the period) differencing ($\nabla_m Y_t = Y_t - Y_{t-m}$) may be used to eliminate / mitigate the seasonal effect.

Ordinary (lag 1) difference ($\nabla Y_t = Y_t - Y_{t-1}$) may still be necessary to eliminate any remaining trend. 

####In the context of a $SARIMA(p,d,q) \times (P,D,Q)_m$ model, explain the distinction between the "within-season" series versus the "between-season" series.

When considering a time series with a seasonal effect of period $m$, we can think of the observations $y_1, y_2, ..., y_{m+1}$ as constituting a "within-season" series that can be modeled by "within-season" orders $(p,q)$. We can also think of $y_{1m}, y_{2m}, y_{3m},...$ as constituting a "between-season" series that can be modeled by "between-season" orders $(P,Q)$.

####In the context of a $SARIMA(p,d,q) \times (P,D,Q)_m$ model, explain how to use ACF and PACF plots to choose $p,q,P,Q$.

We choose $p$ and $q$ by looking at PACF/ACF plots as we normally would, but in this case, we only investigate correlations when $h=1,2,...,s-1$.

We choose $P$ and $Q$ by looking at PACF/ACF plots as we normally would when choosing $p$ and $q$, but in this case, we only investigate correlations when $h=ks, k \in N$.

###Model choice [2016Quiz4Q2/2015Quiz4Q2]

### Unit Root Test

####Explain the purpose of a "unit root test" such as Augmented Dickey Fuller test.

A unit root test formally tests whether an observed time series is stationary. Specifically, it is used to determine whether the "stationary conditions" of an $ARMA$ model are met.

####Explain the "unit root test"'s' relevance in the differencing process.

The null hypothesis is that the time series is non-stationary and so if the null hypothesis is not rejected it suggests the time series be differenced. In practice we keep differencing until the null hypothesis is rejected.

## Holt-Winter exponential smoothing

### Explain for which types of time series is single vs. double vs. triple exponential smoothing appropriate.

* Single exponential smoothing is appropriate for modeling a time series with no trend and no seasonality.
* Double exponential smoothing is appropriate for modeling a time series with trend but no seasonality.
* Triple exponential smoothing is appropriate for modeling a time series with both trend and seasonality.

###Parameter explain

* Smaller values of $\alpha$ make the *level* recursion more smooth.
* Larger values of $\alpha$ make the *level* recursion less smooth.
* Larger values of $\beta$ make the *trend* recursion less smooth.
* Smaller values of $\beta$ make the *trend* recursion more smooth.
* Larger values of $\gamma$ make the *seasonal* recursion less smooth.

## Multivariate

### In the context of multivariate time series, explain what is meant by exogenous and endogenous variables, and in each case state which class of models is more appropriate.

In the context of multivariate time series with a response series and several explanatory time series.

The explanatory ones are called **endogenous** if all variables (the response and explanatory) influence each other. In this case, **VAR** models are appropriate.

The explanatory series  are called **exogenous** if they influence the response but the response doesn't influence them. In this case **SARIMAX** models are appropriate.

### In the context of multivariate time series, explain the distinction between ARIMAX and vector autoregression models. Be sure to indicate when one is to be preferred over the other.

An ARIMAX model accounts for the dependence of the response on the exogenous variables and ignore any possible influence the response has on them.

A VAR model is not uni-directional, it simultaneous accounts for the dependencies between all endogenous variables.

An ARIMAX model is appropriate if you believe a uni-directional relationship exist between one variable and all others and VAR is appropriate if all variables are influence by every other one.

